{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76d1cf4-2753-4127-a51f-7322a7eed9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLLMs can be used in variety of ways for different purposes - \\nIndexing - Relevance of data/Summarize the raw data/Index summaries \\nQuerying - Retrieval And Responce Synthesi \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LLMs can be used in variety of ways for different purposes - \n",
    "Indexing - Relevance of data/Summarize the raw data/Index summaries \n",
    "Querying - Retrieval And Responce Synthesi \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0232b82-7d05-4c85-8dbe-d7cdc213188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api key \n",
    "import os \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4390148c-8f77-4900-8a2a-db319c42838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work in the field of programming languages and software development. Graham is also a prolific writer and has published several influential essays on topics ranging from technology and startups to philosophy and culture.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "response = OpenAI().complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4a50eb-be2a-46d7-92bb-e37b61de7e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the model \n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.2, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# run on your document \n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67702b94-eaa6-473f-9b1e-b89014385fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data \n",
    "\n",
    "'''\n",
    "Ingestion pipeline - three main statges \n",
    "1) Loading data\n",
    "2) Transformation of data\n",
    "3) Index and storing of data \n",
    "'''\n",
    "\n",
    "#loader \n",
    "'''\n",
    "Reader - Data connector\n",
    "Ingest data from different source, and change them into Document. \n",
    "Document - Collection of raw data with metadata - Text/image/audio \n",
    "'''\n",
    "\n",
    "#simple directory reader \n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "'''\n",
    "We can also collect data from SQL Database \n",
    "\n",
    "from llama_index.core import download_loader\n",
    "\n",
    "from llama_index.readers.database import DatabaseReader\n",
    "\n",
    "reader = DatabaseReader(\n",
    "    scheme=os.getenv(\"DB_SCHEME\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASS\"),\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    ")\n",
    "\n",
    "query = \"SELECT * FROM users\"\n",
    "documents = reader.load_data(query=query)\n",
    "\n",
    "More coonectors(to fetch data) on llamahub \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#creating document from scratch \n",
    "from llama_index.core import Document\n",
    "doc = Document(text=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b506a834-157b-4e52-82cc-d8b841353124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation \n",
    "'''\n",
    "Transformation - chunking, metadata extraction, embedding each chunk\n",
    "Input of transformation is node(subclass of document)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#high level transformation, not much control \n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()\n",
    "\n",
    "\n",
    "\n",
    "#custom text splitter - a little change to high level transformation\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# global change \n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = text_splitter\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    transformations=[text_splitter] # new part \n",
    ")\n",
    "index.as_query_engine()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#low level transformation, defining each component as explicitly\n",
    "\n",
    "#first, nodes(chunk) are created by splitting document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "#create documents -- use text splitter -- create nodes\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "#add metadata -- manually or with metadata extractor\n",
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")\n",
    "\n",
    "#add embedding \n",
    "#more on this later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2d3cd6-691e-4e12-afbb-20767eb8c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing \n",
    "\n",
    "'''\n",
    "Index are created over Documents so that they can be used for querying \n",
    "Index is a datastructure composed of Documents. \n",
    "\n",
    "Index Types - \n",
    "1) Vector Store Index - creates vector embedding of each node.\n",
    "    a) Vector embedding - numerical representation of the semantics, or meaning of your text\n",
    "    b) By default LlamaIndex uses text-embedding-ada-002\n",
    "    c) Top_k - The number of embeddings it returns. This is also known as top-k semantic retrieval\n",
    "\n",
    "2) Summary Index - Generate summary of the documents. It returns all the documents in summarized text \n",
    "\n",
    "3) Knowledge Graph Index - If data is interconnected(graph) \n",
    "    1) Converts unstrucutred data into entity based querying \n",
    "'''\n",
    "\n",
    "\n",
    "#vector store indexing \n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents) #first convert docs in nodes then make index\n",
    "\n",
    "\n",
    "#build index over nodes\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674c2be6-0fb4-468e-9979-a51c8465b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storings \n",
    "\n",
    "'''\n",
    "Store the indexed data to avoide re-indexing. By default its stored in local memory \n",
    "\n",
    "'''\n",
    "\n",
    "#store index in particular directory \n",
    "index.storage_context.persist(persist_dir=\"\")\n",
    "\n",
    "#for graph \n",
    "#graph.root_index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "#loading index from memory\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d14b15-57ed-4e0e-ad76-405be6a6dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vector Store \n",
    "\n",
    "QDrant \n",
    "Pincone\n",
    "\n",
    "Explanation is very generic \n",
    "'''\n",
    "\n",
    "#inserting more data(documents/nodes)\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex([])\n",
    "for doc in documents:\n",
    "    index.insert(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3007a6-d97e-4125-8438-722016419eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical risk minimization involves minimizing a certain quantity known as the empirical risk, denoted as Rs(f), over a set of labeled data points. This empirical risk is defined as the average loss over all data points in the set S. Mathematically, it is expressed as Rs(f) = (1/n) * Σ l(f(xi), yi) for i from 1 to n, where n is the number of data points, xi and yi are the input and corresponding output for each data point, and l is the loss function. The process of finding the minimum empirical risk is referred to as optimization within a given function class F.\n"
     ]
    }
   ],
   "source": [
    "# querying \n",
    "\n",
    "'''\n",
    "Most significant part of LLM Application: Querying \n",
    "\n",
    "Just a prompt call to the LLM\n",
    "Complex querying requires repeated/chained prompt or even a reasoning loop across multiple components.\n",
    "\n",
    "'''\n",
    "\n",
    "#simple query # high level API call\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"Explain emperical risk minimization.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfcb1c13-187f-4c93-bd9c-f5e4e792aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical Risk Minimizer (ERM) involves minimizing the empirical risk, which is the average loss over all data points in a given set. This is achieved by finding the function within a function class that minimizes this average loss. The process of minimizing this quantity is referred to as optimization. The goal of ERM is to learn a predictor function that can accurately map input data to output data, enabling predictions for any given input.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stages of query - \n",
    "Retrieval - find and return the most relevant documents\n",
    "Postprocessing - Nodes are reranked/filtered based on the metadata that they have(keywords)\n",
    "Responce Synthesis - Query + Relevant data + Prompt sent to LLM to generate responce\n",
    "'''\n",
    "\n",
    "#low level API call   # Granular control over the data and API\n",
    "# know as retriever \n",
    "\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, #retrieve \n",
    "    response_synthesizer=response_synthesizer, # query + relevant docs\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7) ],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"Explain eperical risk minimizer\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "134dde88-eea7-4973-b681-e9a513be694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define components from scratch \n",
    "\n",
    "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
    "\n",
    "#retriever \n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "\n",
    "#node postpreprocessor \n",
    "'''\n",
    "Improve relevancy of the nodes that fetched\n",
    "\n",
    "1) Keyword - Filter by required keyword or keyword which should be removed \n",
    "2) Similarity - Filter by threshold value/score \n",
    "3) PrevNextNode - augments retrieved node based on neigbours \n",
    "\n",
    "\n",
    "NOTE- THESE METHODS MIGHT RESULT IN EMPTY RESPONCES\n",
    "\n",
    "'''\n",
    "\n",
    "node_postprocessors = [\n",
    "    KeywordNodePostprocessor(\n",
    "        required_keywords=[\"ERM\"], exclude_keywords=[\"\"]\n",
    "    )\n",
    "]\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever, node_postprocessors=node_postprocessors\n",
    ")\n",
    "response = query_engine.query(\"Explain emperical risk minimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5239bb5-54d0-42a5-ba39-12ff2f14ba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical risk minimization involves minimizing a certain quantity known as the empirical risk, denoted as Rs(f), over a set of labeled data points. This empirical risk is defined as the average loss over all data points in the set S. Mathematically, it is expressed as the minimum of Rs(f) over all functions f in the function class F. The process of finding this minimum is referred to as optimization.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec2c6075-bd9b-4570-85b9-e212bf48ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical risk minimization involves minimizing a certain quantity known as the empirical risk, denoted as Rs(f), over a set of labeled data points. This empirical risk is defined as the average loss over all data points in the set S. The goal is to find the function f that minimizes this empirical risk, which is achieved through optimization by selecting the function that yields the minimum empirical risk among all functions in the function class F.\n",
      "True\n",
      "query=None contexts=['The\\nimage\\nexplains\\nthe\\nconcept\\nof\\nEmpirical\\nRisk\\nMinimizer\\n(ERM).\\nIt\\nstarts\\nwith\\na\\ndefinition:\\nGiven\\na\\nfunction\\nclass\\nF,\\nwhich\\nmaps\\nfrom\\nthe\\nset\\nX\\nto\\nthe\\nset\\nY,\\nempirical\\nrisk\\nminimization\\nover\\na\\nset\\nof\\nlabeled\\ndata\\npoints\\nS\\ncorresponds\\nto\\nminimizing\\na\\ncertain\\nquantity.\\nThis\\nquantity\\nis\\nthe\\nempirical\\nrisk,\\ndenoted\\nas\\nRs(f).\\nMathematically ,\\nthis\\nis\\nexpressed\\nas\\nthe\\nminimum\\nof\\nRs(f)\\nover\\nall\\nfunctions\\nf\\nin\\nthe\\nfunction\\nclass\\nF.\\nThe\\nempirical\\nrisk\\nRs(f)\\nis\\ndefined\\nas\\nthe\\naverage\\nloss\\nover\\nall\\ndata\\npoints\\nin\\nthe\\nset\\nS.\\nThis\\nis\\nrepresented\\nby\\nthe\\nformula:\\nRs(f)\\n=\\n(1/n)\\n*\\nΣ\\nl(f(xi),\\nyi)\\nfor\\ni\\nfrom\\n1\\nto\\nn.\\nHere,\\nn\\nis\\nthe\\nnumber\\nof\\ndata\\npoints,\\nxi\\nand\\nyi\\nare\\nthe\\ninput\\nand\\ncorresponding\\noutput\\nfor\\neach\\ndata\\npoint,\\nand\\nl\\nis\\nthe\\nloss\\nfunction.\\nThe\\nprocess\\nof\\nfinding\\nthis\\nminimum\\nis\\nreferred\\nto\\nas\\noptimization,\\nas\\nindicated\\nin\\nred\\ntext.\\nThe\\nimage\\ndescribes\\nthe\\ngeneral\\nframework\\nfor\\nsupervised\\nlearning.\\n\\\\n\\\\nIt\\nbegins\\nby\\ndefining\\nan\\ninput\\nspace,\\nrepresented\\nas\\nX,\\nwhich\\nis\\na\\nsubset\\nof\\nthe\\nreal\\nnumber\\nspace\\nwith\\nd\\ndimensions.\\nThis\\ninput\\nspace\\nconsists\\nof\\ndata\\npoints\\nin\\nd\\ndimensions.\\nIt\\nalso\\nnotes\\nthat\\nin\\na\\nprevious\\nexample,\\nthe\\ndimension\\nd\\nwas\\nequal\\nto\\n1.\\nThere\\nis\\na\\nmention\\nin\\nred\\ntext\\nof\\n\"Feature\\nengineering!\"\\nnext\\nto\\nthe\\ninput\\nspace\\ndefinition.\\\\n\\\\nNext,\\nit\\ndefines\\nan\\noutput\\nspace,\\nrepresented\\nas\\nY.\\nAn\\nexample\\nprovided\\nis\\nY\\nbeing\\nthe\\nset\\nof\\nreal\\nnumbers,\\nwhich\\ncould\\nbe\\nused\\nfor\\nsale\\nprice\\nprediction.\\\\n\\\\nThe\\ngoal\\nof\\nsupervised\\nlearning\\nis\\nthen\\nstated:\\nto\\nlearn\\na\\npredictor\\nfunction\\nf(x)\\nthat\\nmaps\\nthe\\ninput\\nspace\\nX\\nto\\nthe\\noutput\\nspace\\nY.\\nThis\\nfunction\\nshould\\nbe\\nable\\nto\\npredict\\nthe\\noutput\\ncorresponding\\nto\\nany\\ngiven\\ninput\\nx.\\n\\\\n\\\\nHighlighted\\ntext\\nincludes\\n\"input\\nspace,\"\\n\"output\\nspace,\"\\nand\\nthe\\ngoal\\nstatement\\nabout\\nlearning\\na\\npredictor\\nfunction.\\',\\n\\'The\\nimage\\nexplains\\nthe\\nconcept\\nof\\na\\nfunction\\nclass.\\nA\\nfunction\\nclass,\\nalso\\nknown\\nas\\na\\nhypothesis\\nclass,\\nis\\na\\ncollection\\nof\\nfunctions\\ndenoted\\nas\\nf:\\nX\\n->\\nY.\\n\\\\n\\\\nIn\\nthe\\nexample\\nprovided,\\nX\\nand\\nY\\nboth\\nbelong\\nto\\nthe\\nset\\nof\\nreal\\nnumbers\\n(represented\\nby\\nR).\\nThe\\nfunction\\nf\\nis\\ndescribed\\nby\\nthe\\nequation\\ny\\n=\\nwx\\n+\\nc,\\nwhich\\nrepresents\\na\\nlinear\\nfunction.\\\\n\\\\nThe\\nimage\\nincludes\\na\\ngraph\\nshowing\\ndata\\npoints\\nwith\\nsale\\nprice\\non\\nthe\\nvertical\\naxis\\nand\\nsquare\\nfootage\\non\\nthe\\nhorizontal\\naxis.\\nSeveral\\nlinear\\nfunctions\\nare\\ndepicted\\nas\\ndashed\\nlines\\nfitting\\nthrough\\nthe\\ndata\\npoints,\\ndemonstrating\\nthat:\\\\n1.\\nEach\\nof\\nthese\\nlines\\nrepresents\\na\\nlinear\\nfunction.\\\\n2.\\nThe\\ncollection\\nof\\nall\\nthese\\nlinear\\nfunctions\\nforms\\na\\nfunction\\nclass.', \"go\\ntoo\\ndeep\\ninto\\nthis.\\nSo\\nwhen\\nwe\\nhave\\nthis\\ntraining\\na\\ntest\\nerror\\nwe\\ncan\\nwrite\\nas\\npretty\\nsimple\\ntotology.\\nSo\\nwhich\\nit\\ncan\\nbe\\nuseful\\njust\\nas\\na\\nway\\nof\\nthinking\\nabout\\nthings.\\nSo\\nremember\\nour\\ngoal\\nis\\nto\\nminimize\\nthis\\ntrue\\nrisk\\nR\\nof\\nF\\nwhich\\nis\\nthe\\nexpected\\nrisk\\nunder\\nthe\\ndistribution\\nthat\\nis\\nequal\\nto\\nthe\\nfollowing\\njust\\nby\\naddition\\nsubtraction\\nit's\\nthe\\nempirical\\nrisk\\nplus\\nthe\\ntrue\\nrisk\\nminus\\nthe\\nempirical\\nrisk.\\nSo\\nI've\\ndone\\nnothing\\nhere\\nlike\\nthis\\njust\\nfollows\\nbecause\\nthese\\ntwo\\ncancel\\nit's\\nmathematically\\nthere's\\nnothing\\ninteresting\\nhappening\\nhere\\nbut\\nconceptually\\nthis\\nis\\nuseful\\nbecause\\nit\\nsuggests\\nthat\\nif\\nyou\\nwant\\nto\\nminimize\\nthe\\ntrue\\nrisk.\\nSo\\nyou\\ndo\\nthe\\nfollowing\\nyou\\nfirst\\ntry\\nto\\nminimize\\nthe\\nempirical\\nrisk\\nthis\\nR\\nhat\\nS\\nthis\\nempirical\\nrisk\\non\\nthe\\nshading\\nset\\nF\\nS\\nand\\nthen\\nwhat\\nis\\nleft\\nis\\nthis\\nRF\\nminus\\nthe\\nR\\nhat\\nSF\\nso\\nthat's\\nthe\\ntrue\\nrisk\\nminus\\nthe\\nempirical\\nrisk\\nand\\nthat's\\njust\\nthe\\ngeneralization\\ngap.\\nSo\\nwhat's\\nleft\\nis\\nthis\\nRF\\nminus\\nR\\nhat\\nSF\\nwhich\\nis\\nthe\\ngeneralization\\ngap.\\nSo\\ngeneralization\\ngap\\nis\\nthe\\ngap\\nin\\nperformance\\nbetween\\nyour\\ntest\\nset\\nand\\nyour\\ntraining\\nset\\nand\\nremember\\nthe\\ntrue\\nrisk\\nis\\nan\\napproximation\\nof\\nperformance.\\nThe\\ntrue\\nrisk\\ncan\\nbe\\napproximately\\nevaluated\\nby\\nseeing\\nhow\\nwell\\nthe\\nmodel\\ndoes\\non\\na\\ntest\\nset.\\nSo\\nthat's\\nthe\\nerror\\non\\nyour\\ntest\\nset\\nand\\nthis\\nis\\nyour\\nerror\\non\\nyour\\ntraining\\nset\\nthe\\ngap\\nis\\nthe\\ngeneralization\\ngap.\\nSo\\nin\\norder\\nto\\nmake\\nsure\\nthat\\na\\nmodel\\ndoes\\nwell\\non\\nthis\\ndistribution\\nover\\ninstances\\nyou\\nneed\\nto\\ndo\\ntwo\\nthings\\none\\nyou\\nneed\\nto\\ntry\\nto\\nminimize\\nthe\\nempirical\\nrisk\\nmake\\nsure\\nit's\\ndoing\\nwell\\non\\nyour\\ntraining\\nset\\nand\\ntwo\\nmake\\nsure\\nthat\\nis\\ngeneralizing\\nthat\\nthe\\ngap\\nbetween\\nthe\\ntraining\\nand\\ntest\\nerror\\nis\\nsmall.\\nIf\\nboth\\nof\\nthese\\ntwo\\nterms\\nwill\\nbe\\nsmall\\nthen\\nyour\\noverall\\nrisk\\nwill\\nalso\\nbe\\nsmall\\nso\\nyour\\nmodel\\ndo\\nwell\\nwith\\nrespect\\nto\\nthis\\ndistribution\\nof\\ninstances\\nthat\\nyou're\\ninterested\\nin.\\nYeah\\nquestion.\\nSo\\nhow\\ndid\\nI\\ngo\\nfrom\\nthis\\nto\\nthis?\\nSo\\nthis\\nis\\njust\\na\\ndefinition\\nof\\nthis\\nand\\nwe\\ncan\\nevaluate\\nthe\\nexpectation\\nby\\njust\\nfinding\\nthe\\naverage\\nover\\nset\\nof\\ninstances.\\nSame\\nway\\nas\\nthink\\nof\\na\\ncoin\\nif\\nyou\\nwant\\nto\\nevaluate\\nthe\\nexpected\\nexpectation\\nof\\nlike\\nthe\\ncoin\\nlanding\\nof\\nheads\\nyou\\ncan\\nevaluate\\nthat\\nby\\ntaking\\nthe\\nempirical\\naverage\\nso\\naverage\\nwith\\nrespect\\nto\\na\\nlot\\nof\\nsamples\\nfrom\\nthis\\ncoin.\\nThat's\\nthe\\nsame\\nthing\\nwe're\\ndoing\\nhere\\nso\\nwe\\nare\\nestimating\\nsome\\nquantity\\nwith\\nrespect\\nto\\njust\\nsamples\\nand\\nthat's\\nwhy\\nlike\\nall\\nof\\nthese\\nare\\nwe\\ncall\\nthese\\nempirical\\nsamples\\nempirical\\nrisk\\nor\\nempirical\\nrisk\\nminimization\\nbecause\\nthese\\nare\\nall\\nwith\\nrespect\\nto\\nsamples\\nfrom\\nthe\\ndistribution.\"] response='Empirical risk minimization involves minimizing a certain quantity known as the empirical risk, denoted as Rs(f), over a set of labeled data points. This empirical risk is defined as the average loss over all data points in the set S. The goal is to find the function f that minimizes this empirical risk, which is achieved through optimization by selecting the function that yields the minimum empirical risk among all functions in the function class F.' passing=True feedback='YES' score=1.0 pairwise_source=None invalid_result=False invalid_reason=None\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "\n",
    "#for async error - https://github.com/run-llama/llama_index/issues/9978\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "# create llm\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"Explain emperical risk minimization.\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(str(eval_result.passing))\n",
    "\n",
    "#print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54f341-066c-4d21-9353-950ab4ac8767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

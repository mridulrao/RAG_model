{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e514442-8491-4181-913a-23adc24394e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5c489c-3a0e-4b22-93b0-4e325648e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDirectoryReader(\n",
    "    input_files = [\"Lec1_CSCI567.pdf\", \"Slides_Exp.pdf\"],\n",
    "    required_exts = [\".pdf\"],\n",
    "    recursive = True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ad21a35-fc15-426a-9697-0e6c903efe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    string = doc.text\n",
    "    string = string.replace('\\n', ' ')\n",
    "    doc.text = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ddbdc9-9933-4552-a991-61be4b82c3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f876298336984308bedfcf9c9789f094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992a8e54444642c1bff8d471ed6be715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfe75007338463fa77017a3104d7640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afefb674efcc4d3ca026da52c701e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab0524aba024a1d8987d9e5513eedc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b58e50d9eb4408854e1b063c7ae3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9790c055-393b-4f29-8b43-5433c7fa4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56bcf3a4-91f0-4318-ac2a-963895ab6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams, Batch\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "unique_collection_name = \"documents\"\n",
    "vector_store = QdrantVectorStore(client=client, collection_name = unique_collection_name)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c9ae41a-1a9e-4357-9146-e65693b217b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c168eadf-f4f3-4433-a4fd-78b1ebf095d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm=Ollama(model=\"llama3\", request_timeout=90.0, device = device)\n",
    "\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "290984ac-a500-48f9-abef-6f6a2f757e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in detail and also write the refered text in (\"\"), incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d89cf3c-bb66-496c-a0c5-0865b5d38277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information provided, I can attempt to answer your query step by step.\n",
      "\n",
      "ERM stands for Empirical Risk Minimizer. Let's break it down:\n",
      "\n",
      "* \"Empirical\" refers to the fact that we're dealing with a specific set of data points (S) and calculating the average loss over those data points.\n",
      "* \"Risk\" is the expected loss or error of our predictor function f, given the input space X and output space Y. In other words, it's the expected difference between our predicted outputs and the true labels.\n",
      "* \"Minimizer\" means that we're trying to find the optimal (or closest) function in the function class F that minimizes this risk or expected loss.\n",
      "\n",
      "Mathematically, this is expressed as minimizing Rs(f), which is the average loss over all data points in the set S. The formula for Rs(f) is given as:\n",
      "\n",
      "Rs(f) = (1/n) * Î£ l(f(xi), yi) for i from 1 to n\n",
      "\n",
      "where n is the number of data points, xi and yi are the input and corresponding output for each data point, and l is the loss function.\n",
      "\n",
      "So, in summary, ERM refers to the process of finding the optimal (or closest) function in a given function class that minimizes the expected loss or risk over a specific set of labeled data points. ()\n",
      "\n",
      "Referenced text: () \"optimization\" () \"minimum\" () \"Rs(f)\" () \"loss function\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What do we mean by ERM?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce93d6-4989-44b8-aedf-ff4de5e8da46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
